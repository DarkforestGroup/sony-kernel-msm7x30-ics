From 2c448a8cc84318dfdec987d1c35eff426d5a42c8 Mon Sep 17 00:00:00 2001
From: Kiki Rizky <kiki.rizky82@gmail.com>
Date: Tue, 10 Apr 2012 20:20:07 +0700
Subject: [PATCH] update android pmem driver from 2.6.35 kernel [freexperia]

---
 arch/arm/include/asm/mach/map.h |    8 +-
 arch/arm/mm/mm.h                |    7 +-
 drivers/misc/pmem.c             |  431 +++++++++++++++++++++++++++++++++------
 include/linux/android_pmem.h    |   19 ++
 4 files changed, 401 insertions(+), 64 deletions(-)

diff --git a/arch/arm/include/asm/mach/map.h b/arch/arm/include/asm/mach/map.h
index ddff29f..1f93f21 100644
--- a/arch/arm/include/asm/mach/map.h
+++ b/arch/arm/include/asm/mach/map.h
@@ -32,7 +32,13 @@ struct map_desc {
 #ifdef CONFIG_MMU
 extern void iotable_init(struct map_desc *, int);
 
-struct mem_type;
+struct mem_type {
+	unsigned int prot_pte;
+	unsigned int prot_l1;
+	unsigned int prot_sect;
+	unsigned int domain;
+};
+
 extern const struct mem_type *get_mem_type(unsigned int type);
 /*
  * external interface to remap single page with appropriate type
diff --git a/arch/arm/mm/mm.h b/arch/arm/mm/mm.h
index c4f6f05..7b9a6cf 100644
--- a/arch/arm/mm/mm.h
+++ b/arch/arm/mm/mm.h
@@ -15,12 +15,7 @@ static inline pmd_t *pmd_off_k(unsigned long virt)
 	return pmd_off(pgd_offset_k(virt), virt);
 }
 
-struct mem_type {
-	unsigned int prot_pte;
-	unsigned int prot_l1;
-	unsigned int prot_sect;
-	unsigned int domain;
-};
+struct mem_type;
 
 const struct mem_type *get_mem_type(unsigned int type);
 
diff --git a/drivers/misc/pmem.c b/drivers/misc/pmem.c
index 4cfd662..f65b17c 100644
--- a/drivers/misc/pmem.c
+++ b/drivers/misc/pmem.c
@@ -1,7 +1,7 @@
 /* drivers/android/pmem.c
  *
  * Copyright (C) 2007 Google, Inc.
- * Copyright (c) 2009-2010, Code Aurora Forum. All rights reserved.
+ * Copyright (c) 2009-2011, Code Aurora Forum. All rights reserved.
  *
  * This software is licensed under the terms of the GNU General Public
  * License version 2, as published by the Free Software Foundation, and
@@ -19,11 +19,15 @@
 #include <linux/fs.h>
 #include <linux/file.h>
 #include <linux/mm.h>
+#include <linux/mm_types.h>
 #include <linux/list.h>
+#include <linux/vmalloc.h>
+#include <linux/io.h>
 #include <linux/debugfs.h>
 #include <linux/android_pmem.h>
 #include <linux/mempolicy.h>
 #include <linux/kobject.h>
+#include <linux/pm_runtime.h>
 #ifdef CONFIG_MEMORY_HOTPLUG
 #include <linux/memory.h>
 #include <linux/memory_hotplug.h>
@@ -32,7 +36,8 @@
 #include <asm/uaccess.h>
 #include <asm/cacheflush.h>
 #include <asm/sizes.h>
-#include <linux/pm_runtime.h>
+#include <asm/mach/map.h>
+#include <asm/page.h>
 
 #define PMEM_MAX_USER_SPACE_DEVICES (10)
 #define PMEM_MAX_KERNEL_SPACE_DEVICES (2)
@@ -53,6 +58,8 @@
 #define PMEM_DEBUG 0
 #endif
 
+#define SYSTEM_ALLOC_RETRY 10
+
 /* indicates that a refernce to this file has been taken via get_pmem_file,
  * the file should not be released until put_pmem_file is called */
 #define PMEM_FLAGS_BUSY 0x1
@@ -139,6 +146,14 @@ enum pmem_align {
 /* size of unstable PMEM physical memory */
 unsigned long unstable_pmem_size;
 
+struct alloc_list {
+	void *addr;                  /* physical addr of allocation */
+	void *aaddr;                 /* aligned physical addr       */
+	unsigned int size;           /* total size of allocation    */
+	unsigned char __iomem *vaddr; /* Virtual addr                */
+	struct list_head allocs;
+};
+
 struct pmem_info {
 	struct miscdevice dev;
 	/* physical start address of the remaped pmem space */
@@ -158,6 +173,8 @@ struct pmem_info {
 
 	/* index of the garbage page in the pmem space */
 	int garbage_index;
+	/* reserved virtual address range */
+	struct vm_struct *area;
 
 	enum pmem_allocator_type allocator_type;
 
@@ -203,6 +220,11 @@ struct pmem_info {
 				unsigned short quanta;
 			} *bitm_alloc;
 		} bitmap;
+
+		struct {
+			unsigned long used;      /* Bytes currently allocated */
+			struct list_head alist;  /* List of allocations       */
+		} system_mem;
 	} allocator;
 
 	int id;
@@ -222,9 +244,33 @@ struct pmem_info {
 
 	long (*ioctl)(struct file *, unsigned int, unsigned long);
 	int (*release)(struct inode *, struct file *);
+	/* reference count of allocations */
+	atomic_t allocation_cnt;
+	/*
+	 * request function for a region when the allocation count goes
+	 * from 0 -> 1
+	 */
+	void (*mem_request)(void *);
+	/*
+	 * release function for a region when the allocation count goes
+	 * from 1 -> 0
+	 */
+	void (*mem_release)(void *);
+	/*
+	 * private data for the request/release callback
+	 */
+	void *region_data;
+	/*
+	 * map and unmap as needed
+	 */
+	int map_on_demand;
 };
 #define to_pmem_info_id(a) (container_of(a, struct pmem_info, kobj)->id)
 
+static void ioremap_pmem(int id);
+static void pmem_put_region(int id);
+static int pmem_get_region(int id);
+
 static struct pmem_info pmem[PMEM_MAX_DEVICES];
 static int id_count;
 static struct {
@@ -236,7 +282,7 @@ struct pmem_info {
 #ifdef CONFIG_KERNEL_PMEM_SMI_REGION
 	{ PMEM_KERNEL_SMI_DATA_NAME,
 		PMEM_MEMTYPE_SMI,
-		PMEM_MEMTYPE_EBI1,  /* Fall back to EBI1 automatically */
+		PMEM_INVALID_MEMTYPE, /* MUST be set invalid if no fallback */
 		-1 },
 #endif
 	{ PMEM_KERNEL_EBI1_DATA_NAME,
@@ -353,6 +399,8 @@ static ssize_t show_pmem_allocator_type(int id, char *buf)
 		return scnprintf(buf, PAGE_SIZE, "%s\n", "Buddy Bestfit");
 	case  PMEM_ALLOCATORTYPE_BITMAP:
 		return scnprintf(buf, PAGE_SIZE, "%s\n", "Bitmap");
+	case PMEM_ALLOCATORTYPE_SYSTEM:
+		return scnprintf(buf, PAGE_SIZE, "%s\n", "System heap");
 	default:
 		return scnprintf(buf, PAGE_SIZE,
 			"??? Invalid allocator type (%d) for this region! "
@@ -527,11 +575,88 @@ static ssize_t show_pmem_bits_allocated(int id, char *buf)
 	NULL
 };
 
+static struct attribute *pmem_system_attrs[] = {
+	PMEM_COMMON_SYSFS_ATTRS,
+
+	NULL
+};
+
 static struct kobj_type pmem_bitmap_ktype = {
 	.sysfs_ops = &pmem_ops,
 	.default_attrs = pmem_bitmap_attrs,
 };
 
+static struct kobj_type pmem_system_ktype = {
+	.sysfs_ops = &pmem_ops,
+	.default_attrs = pmem_system_attrs,
+};
+
+static int pmem_allocate_from_id(const int id, const unsigned long size,
+						const unsigned int align)
+{
+	int ret;
+	ret = pmem_get_region(id);
+
+	if (ret)
+		return -1;
+
+	ret = pmem[id].allocate(id, size, align);
+
+	if (ret < 0)
+		pmem_put_region(id);
+
+	return ret;
+}
+
+static int pmem_free_from_id(const int id, const int index)
+{
+	pmem_put_region(id);
+	return pmem[id].free(id, index);
+
+}
+
+static int pmem_get_region(int id)
+{
+	/* Must be called with arena mutex locked */
+	atomic_inc(&pmem[id].allocation_cnt);
+	if (!pmem[id].vbase) {
+		DLOG("PMEMDEBUG: mapping for %s", pmem[id].name);
+		if (pmem[id].mem_request)
+				pmem[id].mem_request(pmem[id].region_data);
+		ioremap_pmem(id);
+
+	}
+
+	if (pmem[id].vbase)
+		return 0;
+	else {
+		if (pmem[id].mem_release)
+			pmem[id].mem_release(pmem[id].region_data);
+		atomic_dec(&pmem[id].allocation_cnt);
+		return 1;
+	}
+}
+
+static void pmem_put_region(int id)
+{
+	/* Must be called with arena mutex locked */
+	if (atomic_dec_and_test(&pmem[id].allocation_cnt)) {
+		DLOG("PMEMDEBUG: unmapping for %s", pmem[id].name);
+		BUG_ON(!pmem[id].vbase);
+		if (pmem[id].map_on_demand) {
+			/* unmap_kernel_range() flushes the caches
+			 * and removes the page table entries
+			 */
+			unmap_kernel_range((unsigned long)pmem[id].vbase,
+				pmem[id].size);
+			pmem[id].vbase = NULL;
+			if (pmem[id].mem_release)
+				pmem[id].mem_release(pmem[id].region_data);
+
+		}
+	}
+}
+
 static int get_id(struct file *file)
 {
 	return MINOR(file->f_dentry->d_inode->i_rdev);
@@ -563,7 +688,7 @@ static int has_allocation(struct file *file)
 	 * means that file is guaranteed not to be NULL upon entry!!
 	 * check is_pmem_file first if not accessed via pmem_file_ops */
 	struct pmem_data *pdata = file->private_data;
-	return pdata && pdata->index >= 0;
+	return pdata && pdata->index != -1;
 }
 
 static int is_master_owner(struct file *file)
@@ -579,7 +704,8 @@ static int is_master_owner(struct file *file)
 	master_file = fget_light(data->master_fd, &put_needed);
 	if (master_file && data->master_file == master_file)
 		ret = 1;
-	fput_light(master_file, put_needed);
+	if (master_file)
+		fput_light(master_file, put_needed);
 	return ret;
 }
 
@@ -720,6 +846,7 @@ static int pmem_free_bitmap(int id, int bitnum)
 				curr_bit, curr_bit + curr_quanta);
 			pmem[id].allocator.bitmap.bitmap_free += curr_quanta;
 			pmem[id].allocator.bitmap.bitm_alloc[i].bit = -1;
+			pmem[id].allocator.bitmap.bitm_alloc[i].quanta = 0;
 			return 0;
 		}
 	}
@@ -730,6 +857,27 @@ static int pmem_free_bitmap(int id, int bitnum)
 	return -1;
 }
 
+static int pmem_free_system(int id, int index)
+{
+	/* caller should hold the lock on arena_mutex! */
+	struct alloc_list *item;
+
+	DLOG("index %d\n", index);
+	if (index != 0)
+		item = (struct alloc_list *)index;
+	else
+		return 0;
+
+	if (item->vaddr != NULL) {
+		iounmap(item->vaddr);
+		kfree(__va(item->addr));
+		list_del(&item->allocs);
+		kfree(item);
+	}
+
+	return 0;
+}
+
 static int pmem_free_space_bitmap(int id, struct pmem_freespace *fs)
 {
 	int i, j;
@@ -752,12 +900,12 @@ static int pmem_free_space_bitmap(int id, struct pmem_freespace *fs)
 			const int curr_alloc = pmem[id].allocator.
 						bitmap.bitm_alloc[j].bit;
 			if (curr_alloc != -1) {
+				if (alloc_start == curr_alloc)
+					alloc_idx = j;
 				if (alloc_start >= curr_alloc)
 					continue;
-				if (curr_alloc < next_alloc) {
+				if (curr_alloc < next_alloc)
 					next_alloc = curr_alloc;
-					alloc_idx = j;
-				}
 			}
 		}
 		alloc_quanta = pmem[id].allocator.bitmap.
@@ -778,6 +926,14 @@ static int pmem_free_space_bitmap(int id, struct pmem_freespace *fs)
 	return 0;
 }
 
+static int pmem_free_space_system(int id, struct pmem_freespace *fs)
+{
+	fs->total = pmem[id].size;
+	fs->largest = pmem[id].size;
+
+	return 0;
+}
+
 static void pmem_revoke(struct file *file, struct pmem_data *data);
 
 static int pmem_release(struct inode *inode, struct file *file)
@@ -819,7 +975,7 @@ static int pmem_release(struct inode *inode, struct file *file)
 	/* if it is not a connected file and it has an allocation, free it */
 	if (!(PMEM_FLAGS_CONNECTED & data->flags) && has_allocation(file)) {
 		mutex_lock(&pmem[id].arena_mutex);
-		ret = pmem[id].free(id, data->index);
+		ret = pmem_free_from_id(id, data->index);
 		mutex_unlock(&pmem[id].arena_mutex);
 	}
 
@@ -862,10 +1018,6 @@ static int pmem_open(struct inode *inode, struct file *file)
 	DLOG("pid %u(%s) file %p(%ld) dev %s(id: %d)\n",
 		current->pid, get_task_comm(currtask_name, current),
 		file, file_count(file), get_name(file), id);
-	/* setup file->private_data to indicate its unmapped */
-	/*  you can only open a pmem device one time */
-	if (file->private_data != NULL)
-		return -EINVAL;
 	data = kmalloc(sizeof(struct pmem_data), GFP_KERNEL);
 	if (!data) {
 		printk(KERN_ALERT "pmem: %s: unable to allocate memory for "
@@ -1019,17 +1171,17 @@ static void bitmap_bits_set_all(uint32_t *bitp, int bit_start, int bit_end)
 
 static int
 bitmap_allocate_contiguous(uint32_t *bitp, int num_bits_to_alloc,
-		int total_bits, int spacing, int start_bit)
+		int total_bits, int spacing)
 {
 	int bit_start, last_bit, word_index;
 
 	if (num_bits_to_alloc <= 0)
 		return -1;
 
-	for (bit_start = start_bit; ;
-		bit_start = ((last_bit +
+	for (bit_start = 0; ;
+		bit_start = (last_bit +
 			(word_index << PMEM_32BIT_WORD_ORDER) + spacing - 1)
-			& ~(spacing - 1)) + start_bit) {
+			& ~(spacing - 1)) {
 		int bit_end = bit_start + num_bits_to_alloc, total_words;
 
 		if (bit_end > total_bits)
@@ -1107,8 +1259,7 @@ static int reserve_quanta(const unsigned int quanta_needed,
 	ret = bitmap_allocate_contiguous(pmem[id].allocator.bitmap.bitmap,
 		quanta_needed,
 		(pmem[id].size + pmem[id].quantum - 1) / pmem[id].quantum,
-		spacing,
-		start_bit);
+		spacing);
 
 #if PMEM_DEBUG
 	if (ret < 0)
@@ -1220,6 +1371,64 @@ static int pmem_allocator_bitmap(const int id,
 	return bitnum;
 }
 
+static int pmem_allocator_system(const int id,
+		const unsigned long len,
+		const unsigned int align)
+{
+	/* caller should hold the lock on arena_mutex! */
+	struct alloc_list *list;
+	unsigned long aligned_len;
+	int count = SYSTEM_ALLOC_RETRY;
+	void *buf;
+
+	DLOG("system id %d, len %ld, align %u\n", id, len, align);
+
+	if ((pmem[id].allocator.system_mem.used + len) > pmem[id].size) {
+		DLOG("requested size would be larger than quota\n");
+		return -1;
+	}
+
+	/* Handle alignment */
+	aligned_len = len + align;
+
+	/* Attempt allocation */
+	list = kmalloc(sizeof(struct alloc_list), GFP_KERNEL);
+	if (list == NULL) {
+		printk(KERN_ERR "pmem: failed to allocate system metadata\n");
+		return -1;
+	}
+	list->vaddr = NULL;
+
+	buf = NULL;
+	while ((buf == NULL) && count--) {
+		buf = kmalloc((aligned_len), GFP_KERNEL);
+		if (buf == NULL) {
+			DLOG("pmem: kmalloc %d temporarily failed len= %ld\n",
+				count, aligned_len);
+		}
+	}
+	if (!buf) {
+		printk(KERN_CRIT "pmem: kmalloc failed for id= %d len= %ld\n",
+			id, aligned_len);
+		kfree(list);
+		return -1;
+	}
+	list->size = aligned_len;
+	list->addr = (void *)__pa(buf);
+	list->aaddr = (void *)(((unsigned int)(list->addr) + (align - 1)) &
+			~(align - 1));
+
+	if (!pmem[id].cached)
+		list->vaddr = ioremap(__pa(buf), aligned_len);
+	else
+		list->vaddr = ioremap_cached(__pa(buf), aligned_len);
+
+	INIT_LIST_HEAD(&list->allocs);
+	list_add(&list->allocs, &pmem[id].allocator.system_mem.alist);
+
+	return (int)list;
+}
+
 static pgprot_t phys_mem_access_prot(struct file *file, pgprot_t vma_prot)
 {
 	int id = get_id(file);
@@ -1252,8 +1461,16 @@ static unsigned long pmem_start_addr_bitmap(int id, struct pmem_data *data)
 	return data->index * pmem[id].quantum + pmem[id].base;
 }
 
+static unsigned long pmem_start_addr_system(int id, struct pmem_data *data)
+{
+	return (unsigned long)(((struct alloc_list *)(data->index))->aaddr);
+}
+
 static void *pmem_start_vaddr(int id, struct pmem_data *data)
 {
+	if (pmem[id].allocator_type == PMEM_ALLOCATORTYPE_SYSTEM)
+		return ((struct alloc_list *)(data->index))->vaddr;
+	else
 	return pmem[id].start_addr(id, data) - pmem[id].base + pmem[id].vbase;
 }
 
@@ -1291,6 +1508,18 @@ static unsigned long pmem_len_bitmap(int id, struct pmem_data *data)
 	return ret;
 }
 
+static unsigned long pmem_len_system(int id, struct pmem_data *data)
+{
+	unsigned long ret = 0;
+
+	mutex_lock(&pmem[id].arena_mutex);
+
+	ret = ((struct alloc_list *)data->index)->size;
+	mutex_unlock(&pmem[id].arena_mutex);
+
+	return ret;
+}
+
 static int pmem_map_garbage(int id, struct vm_area_struct *vma,
 			    struct pmem_data *data, unsigned long offset,
 			    unsigned long len)
@@ -1424,10 +1653,14 @@ static void pmem_vma_close(struct vm_area_struct *vma)
 static int pmem_mmap(struct file *file, struct vm_area_struct *vma)
 {
 	struct pmem_data *data = file->private_data;
-	int index;
+	int index = -1;
 	unsigned long vma_size =  vma->vm_end - vma->vm_start;
 	int ret = 0, id = get_id(file);
 
+	if (!data) {
+		pr_err("pmem: Invalid file descriptor, no private data\n");
+		return -EINVAL;
+	}
 #if PMEM_DEBUG_MSGS
 	char currtask_name[FIELD_SIZEOF(struct task_struct, comm) + 1];
 #endif
@@ -1445,8 +1678,7 @@ static int pmem_mmap(struct file *file, struct vm_area_struct *vma)
 	down_write(&data->sem);
 	/* check this file isn't already mmaped, for submaps check this file
 	 * has never been mmaped */
-	if ((data->flags & PMEM_FLAGS_MASTERMAP) ||
-	    (data->flags & PMEM_FLAGS_SUBMAP) ||
+	if ((data->flags & PMEM_FLAGS_SUBMAP) ||
 	    (data->flags & PMEM_FLAGS_UNSUBMAP)) {
 #if PMEM_DEBUG
 		pr_err("pmem: you can only mmap a pmem file once, "
@@ -1456,24 +1688,21 @@ static int pmem_mmap(struct file *file, struct vm_area_struct *vma)
 		goto error;
 	}
 	/* if file->private_data == unalloced, alloc*/
-	if (data && data->index == -1) {
+	if (data->index == -1) {
 		mutex_lock(&pmem[id].arena_mutex);
-		index = pmem[id].allocate(id,
+		index = pmem_allocate_from_id(id,
 				vma->vm_end - vma->vm_start,
 				SZ_4K);
 		mutex_unlock(&pmem[id].arena_mutex);
-		data->index = index;
-		if (data->index < 0) {
+		/* either no space was available or an error occured */
+		if (index == -1) {
 			pr_err("pmem: mmap unable to allocate memory"
 				"on %s\n", get_name(file));
+			ret = -ENOMEM;
+			goto error;
 		}
-	}
-
-	/* either no space was available or an error occured */
-	if (!has_allocation(file)) {
-		ret = -ENOMEM;
-		pr_err("pmem: could not find allocation for map.\n");
-		goto error;
+		/* store the index of a successful allocation */
+		data->index = index;
 	}
 
 	if (pmem[id].len(id, data) < vma_size) {
@@ -1717,6 +1946,21 @@ void flush_pmem_file(struct file *file, unsigned long offset, unsigned long len)
 		goto end;
 
 	vaddr = pmem_start_vaddr(id, data);
+
+	if (pmem[id].allocator_type == PMEM_ALLOCATORTYPE_SYSTEM) {
+		dmac_flush_range(vaddr,
+			(void *)((unsigned long)vaddr +
+				 ((struct alloc_list *)(data->index))->size));
+#ifdef CONFIG_OUTER_CACHE
+		phy_start = pmem_start_addr_system(id, data);
+
+		phy_end = phy_start +
+			((struct alloc_list *)(data->index))->size;
+
+		outer_flush_range(phy_start, phy_end);
+#endif
+		goto end;
+	}
 	/* if this isn't a submmapped file, flush the whole thing */
 	if (unlikely(!(data->flags & PMEM_FLAGS_CONNECTED))) {
 		dmac_flush_range(vaddr, vaddr + pmem[id].len(id, data));
@@ -1905,6 +2149,11 @@ static int pmem_kapi_free_index_bitmap(const int32_t physaddr, int id)
 		bit_from_paddr(id, physaddr) : -1;
 }
 
+static int pmem_kapi_free_index_system(const int32_t physaddr, int id)
+{
+	return 0;
+}
+
 int pmem_kfree(const int32_t physaddr)
 {
 	int i;
@@ -2379,7 +2628,7 @@ static long pmem_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
 			}
 
 			mutex_lock(&pmem[id].arena_mutex);
-			data->index = pmem[id].allocate(id,
+			data->index = pmem_allocate_from_id(id,
 					arg,
 					SZ_4K);
 			mutex_unlock(&pmem[id].arena_mutex);
@@ -2426,9 +2675,9 @@ static long pmem_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
 			}
 
 			mutex_lock(&pmem[id].arena_mutex);
-			data->index = pmem[id].allocate(id,
-					alloc.size,
-					alloc.align);
+			data->index = pmem_allocate_from_id(id,
+				alloc.size,
+				alloc.align);
 			mutex_unlock(&pmem[id].arena_mutex);
 			ret = data->index == -1 ? -ENOMEM :
 				data->index;
@@ -2462,15 +2711,40 @@ static long pmem_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
 
 static void ioremap_pmem(int id)
 {
-	if (pmem[id].cached)
-		pmem[id].vbase = ioremap_cached(pmem[id].base, pmem[id].size);
-#ifdef ioremap_ext_buffered
-	else if (pmem[id].buffered)
-		pmem[id].vbase = ioremap_ext_buffered(pmem[id].base,
-					pmem[id].size);
-#endif
-	else
-		pmem[id].vbase = ioremap(pmem[id].base, pmem[id].size);
+	unsigned long addr;
+	const struct mem_type *type;
+
+	DLOG("PMEMDEBUG: ioremaping for %s\n", pmem[id].name);
+	if (pmem[id].map_on_demand) {
+		addr = (unsigned long)pmem[id].area->addr;
+		if (pmem[id].cached)
+			type = get_mem_type(MT_DEVICE_CACHED);
+		else
+			type = get_mem_type(MT_DEVICE);
+		DLOG("PMEMDEBUG: Remap phys %lx to virt %lx on %s\n",
+			pmem[id].base, addr, pmem[id].name);
+		if (ioremap_page_range(addr, addr + pmem[id].size,
+			pmem[id].base, __pgprot(type->prot_pte))) {
+				pr_err("pmem: Failed to map pages\n");
+				BUG();
+		}
+		pmem[id].vbase = pmem[id].area->addr;
+		/* Flush the cache after installing page table entries to avoid
+		 * aliasing when these pages are remapped to user space.
+		 */
+		flush_cache_vmap(addr, addr + pmem[id].size);
+	} else {
+		if (pmem[id].cached)
+			pmem[id].vbase = ioremap_cached(pmem[id].base,
+						pmem[id].size);
+	#ifdef ioremap_ext_buffered
+		else if (pmem[id].buffered)
+			pmem[id].vbase = ioremap_ext_buffered(pmem[id].base,
+						pmem[id].size);
+	#endif
+		else
+			pmem[id].vbase = ioremap(pmem[id].base, pmem[id].size);
+	}
 }
 
 #ifdef CONFIG_MEMORY_HOTPLUG
@@ -2645,6 +2919,7 @@ int pmem_setup(struct android_pmem_platform_data *pdata,
 	       int (*release)(struct inode *, struct file *))
 {
 	int i, index = 0, kapi_memtype_idx = -1, id, is_kernel_memtype = 0;
+	struct vm_struct *pmem_vma = NULL;
 
 	if (id_count >= PMEM_MAX_DEVICES) {
 		pr_alert("pmem: %s: unable to register driver(%s) - no more "
@@ -2838,6 +3113,35 @@ int pmem_setup(struct android_pmem_platform_data *pdata,
 			pmem[id].size, pmem[id].quantum);
 		break;
 
+	case PMEM_ALLOCATORTYPE_SYSTEM:
+
+#ifdef CONFIG_MEMORY_HOTPLUG
+		goto err_no_mem;
+#endif
+
+		INIT_LIST_HEAD(&pmem[id].allocator.system_mem.alist);
+
+		pmem[id].allocator.system_mem.used = 0;
+		pmem[id].vbase = NULL;
+
+		if (kobject_init_and_add(&pmem[id].kobj,
+				&pmem_system_ktype, NULL,
+				"%s", pdata->name))
+			goto out_put_kobj;
+
+		pmem[id].allocate = pmem_allocator_system;
+		pmem[id].free = pmem_free_system;
+		pmem[id].free_space = pmem_free_space_system;
+		pmem[id].kapi_free_index = pmem_kapi_free_index_system;
+		pmem[id].len = pmem_len_system;
+		pmem[id].start_addr = pmem_start_addr_system;
+		pmem[id].num_entries = 0;
+		pmem[id].quantum = PAGE_SIZE;
+
+		DLOG("system allocator id %d (%s), raw size %lu\n",
+			id, pdata->name, pmem[id].size);
+		break;
+
 	default:
 		pr_alert("Invalid allocator type (%d) for pmem driver\n",
 			pdata->allocator_type);
@@ -2870,22 +3174,35 @@ int pmem_setup(struct android_pmem_platform_data *pdata,
 	if (pmem[id].memory_state == MEMORY_UNSTABLE_NO_MEMORY_ALLOCATED)
 		return 0;
 
-	if (!is_kernel_memtype) {
-		ioremap_pmem(id);
-		if (pmem[id].vbase == 0) {
-			pr_err("pmem: ioremap failed for device %s\n",
-				pmem[id].name);
-			goto error_cant_remap;
+	pmem[id].map_on_demand = pdata->map_on_demand;
+	if (pmem[id].map_on_demand) {
+		pmem_vma = get_vm_area(pmem[id].size, VM_IOREMAP);
+		if (!pmem_vma) {
+			pr_err("pmem: Failed to allocate virtual space for "
+					"%s\n", pdata->name);
+			goto out_put_kobj;
 		}
-	}
-
+		pr_err("pmem: Reserving virtual address range %lx - %lx for"
+				" %s\n", (unsigned long) pmem_vma->addr,
+				(unsigned long) pmem_vma->addr + pmem[id].size,
+				pdata->name);
+		pmem[id].area = pmem_vma;
+	} else
+		pmem[id].area = NULL;
 	pmem[id].garbage_pfn = page_to_pfn(alloc_page(GFP_KERNEL));
+	atomic_set(&pmem[id].allocation_cnt, 0);
+
+	if (pdata->setup_region)
+		pmem[id].region_data = pdata->setup_region();
+
+	if (pdata->request_region)
+		pmem[id].mem_request = pdata->request_region;
+
+	if (pdata->release_region)
+		pmem[id].mem_release = pdata->release_region;
 
 	return 0;
 
-error_cant_remap:
-	if (!is_kernel_memtype)
-		misc_deregister(&pmem[id].dev);
 err_cant_register_device:
 out_put_kobj:
 	kobject_put(&pmem[id].kobj);
diff --git a/include/linux/android_pmem.h b/include/linux/android_pmem.h
index 486ce6a..95c7ef9 100644
--- a/include/linux/android_pmem.h
+++ b/include/linux/android_pmem.h
@@ -101,6 +101,7 @@ enum pmem_allocator_type {
 	 * defined
 	 */
 	PMEM_ALLOCATORTYPE_BITMAP = 0, /* forced to be zero here */
+	PMEM_ALLOCATORTYPE_SYSTEM,
 
 	PMEM_ALLOCATORTYPE_ALLORNOTHING,
 	PMEM_ALLOCATORTYPE_BUDDYBESTFIT,
@@ -155,6 +156,24 @@ struct android_pmem_platform_data
 	unsigned buffered;
 	/* This PMEM is on memory that may be powered off */
 	unsigned unstable;
+	/*
+	 * function to be called when the number of allocations goes from
+	 * 0 -> 1
+	 */
+	void (*request_region)(void *);
+	/*
+	 * function to be called when the number of allocations goes from
+	 * 1 -> 0
+	 */
+	void (*release_region)(void *);
+	/*
+	 * function to be called upon pmem registration
+	 */
+	void *(*setup_region)(void);
+	/*
+	 * indicates that this region should be mapped/unmaped as needed
+	 */
+	int map_on_demand;
 };
 
 int pmem_setup(struct android_pmem_platform_data *pdata,
-- 
1.7.10

